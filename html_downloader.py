"""
HTML Downloader

This module handles downloading HTML content from university pages.
Supports multiple page types: applying, overall-rankings, paying, academics.
Loads university information from universities.json file.
"""

import time
import os
import json
import re
from typing import Optional, List, Dict
from selenium_base import SeleniumBase


class HTMLDownloader(SeleniumBase):
    """Downloads HTML content from US News university pages."""
    
    def __init__(self, headless: bool = True, truncate_at_widget: bool = True, universities_json: str = "universities.json", use_existing_chrome: bool = False, enable_api_collection: bool = False):
        """
        Initialize the HTML downloader with Chrome WebDriver.
        
        Args:
            headless: Whether to run Chrome in headless mode (default: True)
            truncate_at_widget: If True, cut HTML at the blueshift recommendations widget if present
            universities_json: Path to the JSON file containing university information
            use_existing_chrome: Whether to connect to existing Chrome browser (default: False)
            enable_api_collection: Whether to enable API response collection (default: False)
        """
        # API ìˆ˜ì§‘ì´ í•„ìš”í•œ ê²½ìš°ì—ë§Œ ë„¤íŠ¸ì›Œí¬ ëª¨ë‹ˆí„°ë§ í™œì„±í™”
        super().__init__(headless=headless, use_existing_chrome=use_existing_chrome, enable_network_monitoring=enable_api_collection)
        self.truncate_at_widget = truncate_at_widget
        self.downloads_dir = "downloads"
        self.universities_json = universities_json
        self.universities = []
        self.enable_api_collection = enable_api_collection
        # Supported page types
        self.page_types = ["", "overall-rankings", "applying", "paying", "academics", "student-life", "campus-info"]
        
        # Load universities from JSON file
        self.load_universities()
    
    def load_universities(self):
        """Load university information from JSON file."""
        try:
            with open(self.universities_json, 'r', encoding='utf-8') as f:
                self.universities = json.load(f)
            print(f"âœ… Loaded {len(self.universities)} universities from {self.universities_json}")
        except FileNotFoundError:
            print(f"âŒ Universities JSON file not found: {self.universities_json}")
            self.universities = []
        except Exception as e:
            print(f"âŒ Error loading universities JSON: {e}")
            self.universities = []
    
    def extract_university_id_from_link(self, link: str) -> Optional[str]:
        """
        Extract university ID from US News link.
        
        Args:
            link: University link (e.g., "/best-colleges/princeton-university-2627")
            
        Returns:
            University ID as string, or None if not found
        """
        # Pattern to match university ID at the end of the link
        pattern = r'-(\d+)$'
        match = re.search(pattern, link)
        if match:
            return match.group(1)
        return None
    
    def find_university_by_name(self, university_name: str) -> Optional[Dict]:
        """
        Find university information by name.
        
        Args:
            university_name: Name of the university to search for
            
        Returns:
            Dictionary with university info (name, link, id) or None if not found
        """
        university_name_lower = university_name.lower()
        
        for university in self.universities:
            if university_name_lower in university['name'].lower():
                university_id = self.extract_university_id_from_link(university['link'])
                return {
                    'name': university['name'],
                    'link': university['link'],
                    'id': university_id
                }
        
        print(f"âŒ University '{university_name}' not found in the universities list")
        return None
    
    def list_all_universities(self) -> List[str]:
        """Return a list of all university names."""
        return [university['name'] for university in self.universities]
        
    def construct_url_from_link(self, link: str, page_type: str) -> str:
        """
        Construct full URL from university link and page type.
        
        Args:
            link: University link from JSON (e.g., "/best-colleges/princeton-university-2627")
            page_type: Type of page ("", overall-rankings, applying, paying, academics, student-life, campus-info)
            
        Returns:
            Full URL for the requested page
        """
        # All pages use premium.usnews.com as the base URL
        base_url = "https://premium.usnews.com"
        
        # For empty page_type (main page), don't add the slash and page_type
        if page_type == "":
            return f"{base_url}{link}"
        else:
            return f"{base_url}{link}/{page_type}"
    
    def create_downloads_directory(self):
        """Create downloads directory if it doesn't exist."""
        if not os.path.exists(self.downloads_dir):
            os.makedirs(self.downloads_dir)
    
    def generate_filename_and_path(self, university_name: str, page_type: str) -> tuple:
        """
        Generate filename and directory path for the downloaded HTML file.
        
        Args:
            university_name: Name of the university
            page_type: Type of page (applying, overall-rankings, paying, academics)
            
        Returns:
            Tuple of (directory_path, filename)
        """
        # Create safe directory name
        safe_name = university_name.replace(' ', '_').replace('&', 'and').replace(',', '').replace('.', '')
        safe_name = ''.join(c for c in safe_name if c.isalnum() or c in '_-')
        
        # Create university directory
        university_dir = os.path.join(self.downloads_dir, safe_name)
        
        # Simple filename based on page type
        if page_type == "":
            filename = "main.html"
        else:
            filename = f"{page_type.replace('-', '_')}.html"
        
        return university_dir, filename
    
    def download_university_page(self, university_name: str, page_type: str, university_info: Optional[Dict] = None) -> Optional[str]:
        """
        Download HTML content from a specific university page.
        
        Args:
            university_name: Name of the university (will be searched in universities.json)
            page_type: Type of page (applying, overall-rankings, paying, academics)
            university_info: Pre-fetched university info to avoid redundant lookups
            
        Returns:
            Path to the saved HTML file if successful, None otherwise
        """
        if page_type not in self.page_types:
            print(f"âŒ Unsupported page type: {page_type}")
            print(f"Supported types: {', '.join(self.page_types)}")
            return None
        
        # Use provided university info or find it
        if university_info is None:
            university_info = self.find_university_by_name(university_name)
            if not university_info:
                return None
        
        university_id = university_info['id']
        university_link = university_info['link']
        actual_name = university_info['name']
        
        try:
            # Set up the driver if not already set up
            if not self.driver:
                self.setup_driver()
            
            # applying í˜ì´ì§€ì´ê³  API ìˆ˜ì§‘ì´ í™œì„±í™”ëœ ê²½ìš°ì—ë§Œ ëª¨ë‹ˆí„°ë§ íŒ¨í„´ ì¶”ê°€
            if page_type == "applying" and self.enable_api_collection:
                self.add_monitoring_pattern("/best-colleges/compass/api/admissions-calculator?school_id=")
                print("ğŸ“¡ API ì‘ë‹µ ëª¨ë‹ˆí„°ë§ ì‹œì‘...")
            
            # Construct the page URL using the link from JSON
            page_url = self.construct_url_from_link(university_link, page_type)
            
            page_display_name = "main" if page_type == "" else page_type
            print(f"ğŸ“¥ {page_display_name} í˜ì´ì§€ ë‹¤ìš´ë¡œë“œ ì¤‘...")
            
            # ì¬ì‹œë„ ë¡œì§: ìµœëŒ€ 2ë²ˆ ì‹œë„ (ì²« ì‹œë„ + 1ë²ˆ ì¬ì‹œë„)
            max_retries = 1
            retry_count = 0
            
            while retry_count <= max_retries:
                if not self.navigate_to(page_url, wait_time=10):
                    return None
                
                # Check HTTP response status first
                if self.is_error_response():
                    error_type = self.get_error_type()
                    
                    # ì˜êµ¬ì  ì—ëŸ¬ (404, 403 ë“±)ì¸ ê²½ìš° ì¦‰ì‹œ ê±´ë„ˆë›°ê¸°
                    if self.is_permanent_error():
                        print(f"âš ï¸ {page_display_name} í˜ì´ì§€ ê±´ë„ˆëœ€ - {error_type}")
                        return None
                    
                    # ì¼ì‹œì  ì—ëŸ¬ì¸ ê²½ìš° ì¬ì‹œë„
                    if retry_count < max_retries:
                        retry_count += 1
                        print(f"âš ï¸ {page_display_name} í˜ì´ì§€ ì—ëŸ¬ ({error_type}) - 1ë¶„ í›„ ì¬ì‹œë„ ({retry_count}/{max_retries})")
                        time.sleep(60)  # 1ë¶„ ëŒ€ê¸°
                        
                        # ë“œë¼ì´ë²„ ì¬ì‹œì‘ìœ¼ë¡œ ê¹¨ë—í•œ ìƒíƒœì—ì„œ ì¬ì‹œë„
                        self.close()
                        continue
                    else:
                        print(f"âŒ {page_display_name} í˜ì´ì§€ ì¬ì‹œë„ ì‹¤íŒ¨ - {error_type}")
                        return None
                else:
                    # ì—ëŸ¬ê°€ ì—†ìœ¼ë©´ ì •ìƒ ì²˜ë¦¬
                    break
            
            # Get page source after confirming it's not an error
            html_content = self.get_page_source()
            if not html_content:
                return None
            
            # Check for recommendations widget and handle accordingly
            if self.truncate_at_widget:
                cut_index = self._find_widget_cut_index(html_content)
                if cut_index is not None and cut_index > 0:
                    html_content = html_content[:cut_index] + "\n<!-- Truncated before recommendations widget -->\n"
            
            # Create downloads directory
            self.create_downloads_directory()
            
            # Generate directory and filename
            university_dir, filename = self.generate_filename_and_path(actual_name, page_type)
            
            # Create university directory if it doesn't exist
            if not os.path.exists(university_dir):
                os.makedirs(university_dir)
            
            file_path = os.path.join(university_dir, filename)
            
            # Save HTML content
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(html_content)
            
            print(f"âœ… ì €ì¥ë¨: {filename} ({len(html_content):,}ì)")
            
            # applying í˜ì´ì§€ì´ê³  API ìˆ˜ì§‘ì´ í™œì„±í™”ëœ ê²½ìš°ì—ë§Œ API ì‘ë‹µ ì €ì¥
            if page_type == "applying" and self.enable_api_collection:
                api_filename = filename.replace('.html', '_api_responses.txt')
                api_file_path = os.path.join(university_dir, api_filename)
                
                # API ì‘ë‹µ ìˆ˜ì§‘ ë° ì €ì¥
                print(f"ğŸ“¡ API ì‘ë‹µ ìˆ˜ì§‘ ì¤‘... (7ì´ˆ ëŒ€ê¸°)")
                time.sleep(7)  # API í˜¸ì¶œì´ ì™„ë£Œë  ë•Œê¹Œì§€ ì¶©ë¶„íˆ ëŒ€ê¸°
                
                if self.save_api_responses_to_file("/best-colleges/compass/api/admissions-calculator?school_id=", api_file_path):
                    print(f"âœ… API ì‘ë‹µ ì €ì¥ë¨: {api_filename}")
                else:
                    if self.use_existing_chrome:
                        print(f"âš ï¸ ê¸°ì¡´ Chromeì—ì„œëŠ” API ì‘ë‹µ ìˆ˜ì§‘ì´ ì œí•œë©ë‹ˆë‹¤. HTMLì€ ì •ìƒ ì €ì¥ë¨.")
                    else:
                        print(f"âš ï¸ API ì‘ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (í˜ì´ì§€ì— calculatorê°€ ì—†ì„ ìˆ˜ ìˆìŒ)")
            
            # ê°œë³„ í˜ì´ì§€ì—ì„œëŠ” ìºì‹œ ì •ë¦¬í•˜ì§€ ì•ŠìŒ (í•™êµ ë‹¨ìœ„ë¡œ ì •ë¦¬)
            
            return file_path
            
        except Exception as e:
            print(f"âŒ Error downloading {page_type} page: {str(e)}")
            return None
    
    def download_all_pages(self, university_name: str) -> List[str]:
        """
        Download HTML content from all supported university pages.
        
        Args:
            university_name: Name of the university (will be searched in universities.json)
            
        Returns:
            List of paths to saved HTML files
        """
        downloaded_files = []
        
        # Find university information first
        university_info = self.find_university_by_name(university_name)
        if not university_info:
            return downloaded_files
        
        try:
            print(f"ğŸ“š Downloading all pages for {university_info['name']} (ID: {university_info['id']})")
            print("=" * 60)
            
            for i, page_type in enumerate(self.page_types, 1):
                page_display_name = "main" if page_type == "" else page_type
                print(f"\nğŸ“– [{i}/{len(self.page_types)}] Downloading {page_display_name} page...")
                print("-" * 40)
                
                # ê° í˜ì´ì§€ë§ˆë‹¤ ìƒˆë¡œìš´ ë“œë¼ì´ë²„ ì‹œì‘
                self.setup_driver()
                
                # applying í˜ì´ì§€ì´ê³  API ìˆ˜ì§‘ì´ í™œì„±í™”ëœ ê²½ìš°ì—ë§Œ ëª¨ë‹ˆí„°ë§ íŒ¨í„´ ì¶”ê°€
                if page_type == "applying" and self.enable_api_collection:
                    self.add_monitoring_pattern("/best-colleges/compass/api/admissions-calculator?school_id=")
                
                file_path = self.download_university_page(university_name, page_type, university_info)
                if file_path:
                    downloaded_files.append(file_path)
                else:
                    page_display_name = "main" if page_type == "" else page_type
                    print(f"â­ï¸ {page_display_name} í˜ì´ì§€ ê±´ë„ˆëœ€ (í˜ì´ì§€ê°€ ì¡´ì¬í•˜ì§€ ì•Šê±°ë‚˜ ì˜¤ë¥˜ ë°œìƒ)")
                
                # í˜ì´ì§€ ë‹¤ìš´ë¡œë“œ í›„ ë“œë¼ì´ë²„ ì¢…ë£Œ
                self.close()
                
                # Delay between downloads to be respectful to the server
                if i < len(self.page_types):
                    print("â³ Waiting 10 seconds before next download...")
                    time.sleep(10)
            
            print(f"\nğŸ‰ Download Summary:")
            print(f"âœ… Successfully downloaded: {len(downloaded_files)}/{len(self.page_types)} pages")
            
            # í•™êµ ì „ì²´ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ í›„ ìµœì¢… ìºì‹œ ì •ë¦¬
            print(f"ğŸ§¹ {university_info['name']} í•™êµ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ - ìµœì¢… ìºì‹œ ì •ë¦¬ ì¤‘...")
            if self.driver:
                self.clear_cache_and_data()
            
            return downloaded_files
            
        except Exception as e:
            print(f"âŒ Error during batch download: {str(e)}")
            return downloaded_files
            
        finally:
            self.close()

    def _find_widget_cut_index(self, html_content: str):
        """Return the index to cut HTML before the recommendations widget, or None if not found."""
        # ì—¬ëŸ¬ ì¢…ë¥˜ì˜ ì¶”ì²œ ìœ„ì ¯ì„ ì°¾ì•„ì„œ ê°€ì¥ ë¨¼ì € ë‚˜íƒ€ë‚˜ëŠ” ê²ƒ ì„ íƒ
        markers = [
            '<div id="blueshift-recommendations-widget"',
            'id="blueshift-recommendations-widget"',
            'blueshift-recommendations-widget',
            '<div id="null-recommendations-widget"',
            'id="null-recommendations-widget"',
            'null-recommendations-widget',
            'SailthruRecommend__Container'
        ]
        
        earliest_index = None
        for marker in markers:
            idx = html_content.find(marker)
            if idx != -1:
                if earliest_index is None or idx < earliest_index:
                    earliest_index = idx
        
        return earliest_index
    
